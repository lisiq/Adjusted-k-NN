{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset Glass (the same dataset as in table Table 1, row no. 6)\n",
    "df = pd.read_csv('glass0.dat', skiprows=14, names=['col{}'.format(i) for i in range(9)]+['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the white spaces in front of the classes\n",
    "df['class'] = df['class'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    144\n",
       "positive     70\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking which class is scarce\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the distance function\n",
    "def euclidean(a, b):\n",
    "    s = 0\n",
    "    for i in range(len(a)):\n",
    "        s += (a[i] - b[i]) ** 2\n",
    "    return s ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neighbors with their distances\n",
    "def nn(k, x, data):\n",
    "    d = []\n",
    "    for instance in data:\n",
    "        d.append(euclidean(x, instance))\n",
    "    nn = [x for _, x in sorted(zip(d, data))][:k]\n",
    "    d = sorted(d)[:k]\n",
    "    return nn, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges the neighbours and sorts them by distances\n",
    "def sortedMerge(nn_p, d_p, nn_m, d_m):\n",
    "    nn = nn_p + nn_m\n",
    "    d = d_p + d_m\n",
    "    nn = [x for _, x in sorted(zip(d, nn))]\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the first k elements of a list\n",
    "def firstK(k, sM):\n",
    "    return sM[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification of a new example with gamma k-NN\n",
    "def gammakNN(x, positive_data, negative_data, k, gamma, distance_func):\n",
    "    nn_minus, d_minus = nn(k, x, negative_data) # nearest negative neighbors with their distances\n",
    "    nn_plus, d_plus = nn(k, x, positive_data) # nearest negative neighbors with their distances\n",
    "    d_plus = [gamma*i for i in d_plus]\n",
    "    nn_gamma = firstK(k, sortedMerge(nn_plus, d_plus, nn_minus, d_minus))\n",
    "    if len([x for x in nn_gamma if x in nn_plus]) >= k/2: # majority vote based on NN_{\\gamma}\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the experiments in the paper the data was randomely split with 80% of the data\n",
    "# for the training data and 20% for the testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size = 0.2, random_state=1)\n",
    "\n",
    "# the datasets were normalized using min-max normalization where the features are in [-1,1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X_train.iloc[:,:-1] = scaler.fit_transform(X_train.iloc[:,:-1])\n",
    "X_test.iloc[:,:-1] = scaler.transform(X_test.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters were tuned with a 10-fold closs-validation over the training set\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# splitting the training set into 10 folds\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# we save the F-measure from all the iterations \n",
    "f_measure = []\n",
    "\n",
    "# gamma was selected from the interval [0, 1] using a step of 0.1\n",
    "for gamma in list(np.arange(0,1,.1)):\n",
    "    Y = []\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X = X_train.iloc[test_index,:] # test set within the cross-validation\n",
    "        positive_data = X_train.iloc[train_index,:][X_train.iloc[train_index,:]['class'] == 'positive'].iloc[:,:-1].values.tolist()\n",
    "        negative_data = X_train.iloc[train_index,:][X_train.iloc[train_index,:]['class'] == 'negative'].iloc[:,:-1].values.tolist()\n",
    "        y = [] # predicted class for the test set\n",
    "        for x in X.iloc[:,:-1].values.tolist():\n",
    "            y.append(gammakNN(x, positive_data, negative_data, 3, gamma, euclidean))\n",
    "        Y.append(f1_score(y, X.iloc[:,-1].values.tolist(),pos_label=\"positive\"))\n",
    "    f_measure.append(np.mean(Y)) \n",
    "\n",
    "# selecting the best gamma that offered the best result\n",
    "best_gamma = [x for _, x in sorted(zip(f_measure, list(np.arange(0,1,.1))))][-1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chosen gamma from hyperparameter tuning \n",
    "best_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-measure is: 0.7555555555555555\n"
     ]
    }
   ],
   "source": [
    "# now training with the whole training set using the best gamma\n",
    "# in the experiments k was 3\n",
    "positive_data = X_train[X_train['class'] == 'positive'].iloc[:,:-1].values.tolist()\n",
    "negative_data = X_train[X_train['class'] == 'negative'].iloc[:,:-1].values.tolist()\n",
    "\n",
    "for experiment in range(5):\n",
    "    y = []\n",
    "    for x in X_test.iloc[:,:-1].values.tolist():\n",
    "        y.append(gammakNN(x, positive_data, negative_data, 3, best_gamma, euclidean))\n",
    "        \n",
    "print('F-measure is: {}'.format(f1_score(y, X_test.iloc[:,-1].values.tolist(),pos_label=\"positive\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
